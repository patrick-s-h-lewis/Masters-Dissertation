\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{unsrtnat}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{3}{chapter*.2}}
\citation{word2vec2}
\citation{word2veckingqueen}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Modern Scientific Publishing}{5}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{5}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Aims}{5}{section.1.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Data Acquisition}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapt:DATA_ACQUISITION}{{2}{7}{Data Acquisition}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Background}{7}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}HTML and Xpath}{7}{subsection.2.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Tree representation of HTML code. The html code here displays a table with 3 rows. The page has two peices of metadata associated with it, stored in the `head'.}}{7}{figure.2.1}}
\newlabel{fig:HTMLTREE}{{2.1}{7}{Tree representation of HTML code. The html code here displays a table with 3 rows. The page has two peices of metadata associated with it, stored in the `head'}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Automatic Xpath Generation}{8}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Collection Strategy}{8}{section.2.3}}
\citation{CROSSREF-FORMATION}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Document Object Identifiers}{9}{subsection.2.3.1}}
\newlabel{sec:DOI}{{2.3.1}{9}{Document Object Identifiers}{subsection.2.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Doi structure. The structure consists of a numeric prefix (X and Y must be integers) and alphanumeric suffix (Z can be any Unicode encoded character) }}{9}{figure.2.2}}
\newlabel{fig:DOI}{{2.2}{9}{Doi structure. The structure consists of a numeric prefix (X and Y must be integers) and alphanumeric suffix (Z can be any Unicode encoded character)}{figure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Perl Syntax Code that can identify the vast majority of DOIs within free text) }}{10}{figure.2.3}}
\newlabel{fig:REGEX}{{2.3}{10}{Perl Syntax Code that can identify the vast majority of DOIs within free text)}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Scraping Program}{10}{subsection.2.3.2}}
\newlabel{sec:SCRAPING_PROGRAM}{{2.3.2}{10}{Scraping Program}{subsection.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The data flow of the scraping program. An inputted list of websites to scrape are visited and dois are extracted in the process described in \ref  {sec:DOI}. The Crossref API service is then used to verify the extracted dois, and collects available meta-data. The program then accesses publisher webpages and collects the abstracts. The program also produces explanation of capture failures and some general statistics}}{11}{figure.2.4}}
\newlabel{fig:Cherry}{{2.4}{11}{The data flow of the scraping program. An inputted list of websites to scrape are visited and dois are extracted in the process described in \ref {sec:DOI}. The Crossref API service is then used to verify the extracted dois, and collects available meta-data. The program then accesses publisher webpages and collects the abstracts. The program also produces explanation of capture failures and some general statistics}{figure.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Collection Results}{12}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}UK University Department scraping}{12}{subsection.2.4.1}}
\newlabel{sec:UKSCRAPE}{{2.4.1}{12}{UK University Department scraping}{subsection.2.4.1}{}}
\newlabel{tab:UKSCRAPERES}{{2.4.1}{12}{UK University Department scraping}{subsection.2.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces UK Scraping results}}{12}{table.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The loss processes are coloured red, successfully captured full records in green, and the maximum possible yield in blue.}}{13}{figure.2.5}}
\newlabel{fig:UKSANK}{{2.5}{13}{The loss processes are coloured red, successfully captured full records in green, and the maximum possible yield in blue}{figure.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Very Large Scale Scraping}{13}{subsection.2.4.2}}
\newlabel{sec:CROSSREFSCRAPE}{{2.4.2}{13}{Very Large Scale Scraping}{subsection.2.4.2}{}}
\newlabel{sec:CROSSREFSCRAPE}{{2.4.2}{14}{Very Large Scale Scraping}{subsection.2.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Problems with ACS and Taylor and Francis}{14}{subsection.2.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The request frequency is plotted in blue, the received pages frequency in red. The vertical dashed line shows where the server detected the scape and banned the IP.}}{15}{figure.2.6}}
\newlabel{fig:ACSBAN}{{2.6}{15}{The request frequency is plotted in blue, the received pages frequency in red. The vertical dashed line shows where the server detected the scape and banned the IP}{figure.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Analysis of Collected data}{15}{subsection.2.4.4}}
\newlabel{tab:LARGESCRAPERES}{{2.4.4}{16}{Analysis of Collected data}{subsection.2.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Large Scale Scraping Results}}{16}{table.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The loss processes are coloured red, successfully captured full records in green, and the maximum possible yield in blue.}}{16}{figure.2.7}}
\newlabel{fig:LARGESANK}{{2.7}{16}{The loss processes are coloured red, successfully captured full records in green, and the maximum possible yield in blue}{figure.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Blue databases represent data with dois and metadata. Green databases represent meta-data, dois and abstracts. The purple database is the combined complete records, and the red database is the data deemed suitable for the training algorithm. database sizes and losses are annotated.}}{17}{figure.2.8}}
\newlabel{fig:DATABASES}{{2.8}{17}{Blue databases represent data with dois and metadata. Green databases represent meta-data, dois and abstracts. The purple database is the combined complete records, and the red database is the data deemed suitable for the training algorithm. database sizes and losses are annotated}{figure.2.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Observations}{17}{section*.3}}
\newlabel{sec:CORPUSOBSERVATIONS}{{2.4.4}{17}{Observations}{section*.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Articles grouped by publisher in the Large Scale Scrape doi database. Only the top 12 publishers are shown.}}{18}{figure.2.9}}
\newlabel{fig:PUBPI}{{2.9}{18}{Articles grouped by publisher in the Large Scale Scrape doi database. Only the top 12 publishers are shown}{figure.2.9}{}}
\citation{zipf}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Articles grouped by publisher in the UK Doi database published by by each publisher. Only the top 12 publishers are shown.}}{19}{figure.2.10}}
\newlabel{fig:UKPUBPI}{{2.10}{19}{Articles grouped by publisher in the UK Doi database published by by each publisher. Only the top 12 publishers are shown}{figure.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces The log Frequency of words vs the log of their position in the rank in the word frequency table in blue. Best fit line in Red, gradient = -1.11, intercept 6.3. }}{19}{figure.2.11}}
\newlabel{fig:ZIPF}{{2.11}{19}{The log Frequency of words vs the log of their position in the rank in the word frequency table in blue. Best fit line in Red, gradient = -1.11, intercept 6.3}{figure.2.11}{}}
\newlabel{sec:SCRAPEANALYSIS}{{2.4.4}{19}{Observations}{table.2.3}{}}
\newlabel{tab:CORPUS STATS}{{2.4.4}{20}{Observations}{figure.2.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Titles and Abstracts in Training Database}}{20}{table.2.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Techniques for Language Processing}{21}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Background}{21}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Bag of Words}{21}{section.3.2}}
\newlabel{tab:BAGOFWORDS}{{3.2}{21}{Bag of Words}{section.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Bag of words}}{21}{table.3.1}}
\citation{WORD2VECKINGQUEEN}
\citation{olddistributed}
\citation{word2vec1}
\citation{word2vec2}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Bag of Citations}{22}{section.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Word2Vec}{22}{section.3.4}}
\newlabel{sec:WORD2VEC}{{3.4}{22}{Word2Vec}{section.3.4}{}}
\citation{word2vec2}
\citation{word2veckingqueen}
\citation{word2vec2}
\citation{word2veckingqueen}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The training architectures of the Word2Vec training algorithm. Word vectors are denoted $v(i)$ for word i. In CBOW word i is predicted by the vector found by summing vectors surrounding i, and $v(i)$ is adjusted to be closer to this prediction. In skip-gram, word i's vector is pairwise compared to its context words, here i-1 and i+1 as a basis to improve $v(i)$.)}}{23}{figure.3.1}}
\newlabel{fig:CBOWSKIP}{{3.1}{23}{The training architectures of the Word2Vec training algorithm. Word vectors are denoted $v(i)$ for word i. In CBOW word i is predicted by the vector found by summing vectors surrounding i, and $v(i)$ is adjusted to be closer to this prediction. In skip-gram, word i's vector is pairwise compared to its context words, here i-1 and i+1 as a basis to improve $v(i)$.)}{figure.3.1}{}}
\citation{gensim}
\citation{doc2vec}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Schematic Representation of how concepts can be represented in word vector space. Word2Vec is able to replicate this behaviour. The vector found by vec(‘King’)- vec(‘Man’)+vec(‘Woman’) is approximately equal to vec(‘Queen’). The model has been tested on thousands of similar examples\cite  {word2vec2}\cite  {word2veckingqueen}.}}{24}{figure.3.2}}
\newlabel{fig:KINGQUEEN}{{3.2}{24}{Schematic Representation of how concepts can be represented in word vector space. Word2Vec is able to replicate this behaviour. The vector found by vec(‘King’)- vec(‘Man’)+vec(‘Woman’) is approximately equal to vec(‘Queen’). The model has been tested on thousands of similar examples\cite {word2vec2}\cite {word2veckingqueen}}{figure.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Doc2Vec}{24}{section.3.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Algorithm Development}{25}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapt:ALGORITHM}{{4}{25}{Algorithm Development}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Premise}{25}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Data Sanitisation}{25}{section.4.2}}
\citation{NLTK}
\citation{Porter}
\citation{porter}
\citation{snowball}
\citation{nltk}
\citation{lancaster}
\citation{wordnet1}
\citation{wordnet2}
\citation{wordnet3}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces All the punctuation removed in scraping. Only these were found in appreciable quantities in the training dataset. }}{26}{figure.4.1}}
\newlabel{fig:punct}{{4.1}{26}{All the punctuation removed in scraping. Only these were found in appreciable quantities in the training dataset}{figure.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Chemistry stopwords}}{26}{table.4.1}}
\citation{sanex}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Stemming}}{27}{table.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces All documents in the training database were preprocessed with this pipeline schema before being used in training models}}{27}{figure.4.2}}
\newlabel{fig:SANPIPE}{{4.2}{27}{All documents in the training database were preprocessed with this pipeline schema before being used in training models}{figure.4.2}{}}
\citation{gensim}
\citation{gensim}
\citation{gensim}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Word2Vec Models}{28}{section.4.3}}
\newlabel{tab:hyperparams}{{4.3}{28}{Word2Vec Models}{section.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Word2vec Parameters}}{28}{table.4.3}}
\citation{doc2vec}
\citation{gensim}
\citation{gensim}
\citation{doc2vec}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}TF-IDF}{29}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Aggregations}{29}{subsection.4.3.2}}
\newlabel{tab:hyperparams}{{4.3.2}{29}{Aggregations}{subsection.4.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Word2vec Document Vector Models}}{29}{table.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Doc2Vec Models}{29}{section.4.4}}
\newlabel{tab:hyperparams}{{4.4}{30}{Doc2Vec Models}{section.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Word2vec Parameters}}{30}{table.4.5}}
\citation{word2vec}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Model Examination}{31}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Word Similarities}{31}{section.5.1}}
\citation{word2vec1}
\citation{word2vec2}
\citation{doc2vec}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Closest words to test words using cosine similarity}}{32}{table.5.1}}
\newlabel{tab:COSINESIMS}{{5.1}{32}{Closest words to test words using cosine similarity}{table.5.1}{}}
\newlabel{tab:EUCLIDSIMS}{{5.1}{32}{Word Similarities}{table.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Closest words to test words using Euclidean similarity}}{32}{table.5.2}}
\citation{docassay}
\citation{docassay}
\citation{PCA}
\citation{TSNE}
\citation{scikitlearn}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Document Similarities}{33}{section.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Document Vector Similarities to \cite  {docassay}}}{33}{table.5.3}}
\newlabel{tab:DOCSIMS}{{5.3}{33}{Document Vector Similarities to \cite {docassay}}{table.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Visualisation Techniques}{34}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Network Visualisation}{34}{subsection.5.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces PCA map of 10,000 documents in the corpus. PCA has not any particular structure. The dimensional reduction task is probably too difficult for PCA.}}{34}{figure.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces TSNE map of the same 10,000 documents. Document vectors have gathered into noticeable clusters, with non negligible outlier documents between clusters. }}{34}{figure.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Networks and Network Visualisation}{35}{subsection.5.3.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Analysis with Sample Dataset}{36}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusions}{37}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibdata{References}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Recommendations for Further Work}{38}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{word2vec2}{{1}{2013{a}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{word2veckingqueen}{{2}{2013{b}}{{Mikolov et~al.}}{{Mikolov, tau Yih, and Zweig}}}
\bibcite{zipf}{{3}{2010}{{Ullah and Giles}}{{}}}
\bibcite{word2vec1}{{4}{2013{c}}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{gensim}{{5}{2010}{{{\v R}eh{\r u}{\v r}ek and Sojka}}{{}}}
\bibcite{doc2vec}{{6}{2014}{{Le and Mikolov}}{{}}}
\bibcite{NLTK}{{7}{2009}{{Bird et~al.}}{{Bird, Klein, and Loper}}}
\bibcite{Porter}{{8}{1980}{{Porter}}{{}}}
\bibcite{lancaster}{{9}{1990}{{Paice}}{{}}}
\bibcite{wordnet1}{{10}{2016}{{Feinerer and Hornik}}{{}}}
\bibcite{wordnet2}{{11}{2007}{{Wallace}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{39}{chapter*.4}}
\bibcite{wordnet3}{{12}{1998}{{Fellbaum}}{{}}}
\bibcite{sanex}{{13}{2007}{{Seesukphronrarak and Takata}}{{}}}
\bibcite{docassay}{{14}{2012}{{Tsaplev}}{{}}}
\bibcite{PCA}{{15}{1901}{{Pearson}}{{}}}
\bibcite{TSNE}{{16}{2008}{{van~der Maaten}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Appendix}{41}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}UK Departments scraped}{42}{section.9.1}}
