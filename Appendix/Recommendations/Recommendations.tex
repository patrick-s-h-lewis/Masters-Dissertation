\label{chapt:RECOMMENDATIONS}
As alluded to in the text, there are several recommendations for further work. The code and data will be improved and amended over time, and is freely available under MIT licence on request \footnote{A digital copy is included with this dissertation.}. If attempting to carry out further work on this project, it is recommended to contact the author for in-depth explanations. This list is by no means exhaustive, and it is the author's belief that literature semantic analysis should be considered an important analytical chemical tool.
\subsection{Greater Dimensionality and Training Improvements}
The principles behind the methods discussed in the project have been shown to be sound. Models should now be improved. Computing resources should be obtained to train higher dimensional vectors \footnote{ The author recommends 400 dimensional vectors}. The models should also be trained for longer ($> 24$) epochs on more data ($> 460000$ documents). These steps will lead to more expressive models.
\subsection{Greater use of word vectors}
This project focussed mainly on document vectors. However, word vectors may be very useful. A method for testing the quality of improved models should be developed. This could take the form of expected relationships to test the model: e.g. Fluorine is to Fluoride as Chlorine is to ... . Many hundreds of these relationships should be systematically built up to test model intuition.\footnote{This would probably require much larger, more descriptive training sets, e.g. textbook transcipts etc.} This follows the methodologies set out in the literature \cite{word2vec1} \cite{word2vec2}. Furthermore, is it possible to predict chemical properties using semantic relationships found in the literature? Vec(Compound A) + Vec(Compound B) + Vec(Lab Technique) may give vec(Product C). If so, it may be possible to find unexpected reactions. This could be coupled with the RInChI database to form a new type of data-driven cheminformatics.
\subsection{Time resolution in clustering}
Methods have been described for clustering documents. The cluster centres represent the content of the cluster effectively. By finding early papers in the cluster, is it possible to identify influential papers or authors?
By clustering on documents from particular years, is it possible to identify a path for the evolving cluster centre vector? If so, it should be possible to extrapolate to \emph{predict} near future research directions.
\subsection{Open Source Chemistry Vectors}
With the increase in open source papers, it should be possible to build up a vast dataset of chemical language for training, using the bodies of articles published on open source platforms, and even to use supplied supporting information. 
\subsection{Structure stemming}
Chemical names could be smartly preprocessed to classes of chemicals, for example by identifying a compound from its name and mapping to InChI key, then to a chemical class. This would allow better association of chemical fragments in training.
\subsection{Multiply labelled Documents}
In Training Doc2Vec, by specifying document with more than just their unique identifiers allows more vectors to be associated. By identifying and labelling all documents with a particular concept, e.g. `palladium-catalysed', and then training Doc2Vec, one defines an 'palladium-catalysed' vector, specifically trained for the concept. These concept vectors would be robust and information-rich\footnote{e.g. which documents are close to the indium-catalysed vector but do not contain the word indium...}
\subsection{Technical Details}
In the interest of future work, this section details the technical details of artefacts provided with this project. The code used in this project was written in a largely self-documenting style. The time limits did not permit for professional doc-strings to be produced, or for anaconda packages to be provided, but the code is well commented. There is also a comprehensive set of JuPyTer Notebooks as tutorial guides for using the code. The core code has been presented in a `package' style.  The module was named \texttt{fruitbowl} with four submodules,
\begin{itemize}
\item \texttt{Cherry} for operations concerning scraping and data collection.
\item \texttt{Orange} for operations concerning NLP corpus creation and big data memory-friendly streaming
\item \textt{Strawberry} for operations concerning Model Training
\item \textt{Apple} for operations concerning analysis of trained models (visualisation, export management etc.)
\end{itemize} 
The list of dependencies required for fully functional behaviour for the \texttt{fruitbowl} suite is as follows:
\begin{itemize}
\item \texttt{Python 2.7} Developed on Python 2.7.11 (recommended version)
\item Python 2 external modules required:
	\begin{itemize}
	\item \texttt{matplotlib 1.5.1} Plotting modules
	\item \texttt{Seaborn 0.7.0} Extension to plotting modules and data analysis \cite{seaborn}
    \item \textt{numpy 1.10.4} Computational Library	
	\item \texttt{Scikit-Learn 0.17} Machine learning library \cite{scikit-learn}
	\item \textt{Scrapy 1.0.3} Scraping framework
	\item \texttt{Gensim 0.12.2} Natural Langauge Processing library
	\item \texttt{nltk 3.1} Natural Language ToolKit library
	\item \texttt{pandas 0.17.1} Data analysis and management library 
	\item \texttt{pymongo 3.0.3} Python driver for MongoDB database
    \item \texttt{requests 2.9.1} Web scraping library
    \item \texttt{scipy 0.17.0} Scientific computing library
	\item \texttt{jupyter 1.0.0} Jupyter notebooks will be required to use the tutorial notebooks.
    \end{itemize}

\item \texttt{JDK} Java Development Kit - for Gephi graph analysis via gephi api
\item \texttt{apache-maven-3.3.9} Java dependency manager - for Gephi graph analysis via gephi api
\item \texttt{C Compiler} for use in BHTSNE reductions\cite{bhtsne}.
\item \texttt{mongoDB} The program was built around use of MongoDB. Not strictly necessary but strongly recommended. Recommended versions $>$3.2.
\end{itemize}  

Data used in the project is also supplied in .json format. The data provided is as follows:
\begin{itemize}
\item \texttt{Delta1.json} - These are the DOIs found in the UK scrape
\item \texttt{Delta2.json} - These are the complete meta-data results found in the UK scrape
\item \texttt{Delta3.json} - These are the DOIs found in the global scrape
\item \texttt{Delta4.json} - These are the complete meta-data results found in the global scrape
\item \texttt{Delta6.json} - This is the data used for training and analytical purposes in the project
\item \texttt{Delta7.json} - The subset of $\Delta6$ from Cambridge used in \S\ref{chapt:ANALYSIS}
\item \texttt{cbow_model} - Gensim binary saved model for final cbow Word2Vec model used in the project
\item \texttt{sg_model} - Gensim binary saved model for final skipgram Word2Vec model used in the project
\item \texttt{FULL_DOC2VEC} - Gensim binary saved model for final Doc2Vec model used in the project
\end{itemize}
Note $Delta5$ is not provided to save disk space (It is simply $\Delta2$+$\Delta4$).