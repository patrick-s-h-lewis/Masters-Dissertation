\chapter{Conclusions}
Focussing first on the data acquisition phase, the scraping procedure was regarded as modest success. The volumes of data collected from the UK Chemistry departments was respectable, as was conversion rate from the potential results to fully resolved records (72.9\%  to give a database of 16363). The actual number of articles from UK chemistry departments can be confidently predicted to be considerably larger. The limited harvest could be down to the input list of scraping websites being too small. The procedure to identify webpages for scraping was limited where the chemistry departments did not host their own website. This  precluded large parts of many important departments. Identifying potential webpages to scrape could have been implemented more effectively.  However, the data that was successfully resolved was high relevance, with few false positive inclusions. The scraping program was robust and efficient, and performed well. 

The large-scale scraping can be regarded as a success as the data collected was sufficiently populous and chemistry-specific  to enable effective models to be trained. It should also be highlighted all of datasets created were from freely available sources, requiring no subscription and could be collected by anyone. This said, it must be acknowledged that most publishers discourage automatic scraping, and the publisher banning was considered as a major failure in the project. That said, it was dealt with swiftly, and did not present a lasting issue. \footnote{The author wishes to thank the librarians and Professor Goodman for efficiently addressing the problem}.

 It should be mentioned that there are existing metadata stores available (such as PubMed). Whilst using one of these datasets would certainly have been easier to use, (and  are considerably larger), there was no real available dataset spanning \emph{chemistry} with enough width of data. The Training dataset, whilst taking considerable time and effort to create, was heterogeneous and thus was a more suitable tool.

The algorithmic development section can be regarded as successful. The premise of quantitative vectorial representation of chemical articles was realised, especially by the doc2vec model. It should be mentioned the TF-IDF models failed to produce well behaved vectors, which is not well understood. The success of the model can begin to be seen in the Analysis section, where it's clustering performances were intuitive and instructive. The potential of the models has not fully explored. It is the author's opinion that another project could be filled developing further uses of the training data set and extending the methodologies presented. 
Some model design choices may have limited specificity, such as the decision to use 100 dimensional vectors for computational tractability.\footnote{Higher dimensional vectors have been shown to perform better}.

The analysis that was performed is most interesting, but the usage to chemists is somewhat limited. As a chemical project, it should have been a strong focus to produce results directly useful to chemistry. This was achieved to some extent towards the end of the project, but this point was reached probably slightly too late.

Some further useful applications of the methodologies have been alluded to, but most of these take the form of a \emph{service} rather than concrete universal insight. Whilst the author would be enthusiastic to implement some of these services (On demand similarities, clustering, recommendations of articles to read, reseach profiling etc.), the project scope had to be limited somewhere.

It is concluded that the aims set out in this project have been addressed, and there were no major barriers preventing the fulfilment of the project brief. 