\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Tree representation of HTML code. The html code here displays a table with 3 rows. The page has two peices of metadata associated with it, stored in the `head'.}}{8}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Doi structure. The structure consists of a numeric prefix (X and Y must be integers) and alphanumeric suffix (Z can be any Unicode encoded character) }}{10}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Perl Syntax Regex Code that can identify the vast majority of DOIs within free text) }}{11}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces The data flow of the scraping program. Websites from an inputted list of websites are visited and dois are extracted in the process described in section \ref {sec:DOI}. The Crossref API service is then used to verify the extracted dois, and collects available meta-data. The program then accesses publisher webpages and collects abstracts. The program also produces explanation of capture failures and some general statistics}}{12}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces The loss processes are coloured red, successfully captured full records in green, and the maximum possible yield in blue.}}{14}{figure.2.5}
\contentsline {figure}{\numberline {2.6}{\ignorespaces The request frequency is plotted in blue, the received pages frequency in red. The vertical dashed line shows where the server detected the scape and banned the IP.}}{16}{figure.2.6}
\contentsline {figure}{\numberline {2.7}{\ignorespaces The loss processes are coloured red, successfully captured full records in green, and the maximum possible yield in blue.}}{17}{figure.2.7}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Blue databases represent data with dois and metadata. Green databases represent meta-data, dois and abstracts. The purple database is the combined complete records, and the red database is the data deemed suitable for the training algorithm. database sizes and losses are annotated.}}{18}{figure.2.8}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Articles grouped by publisher in the Large Scale Scrape doi database. Only the top 12 publishers are shown.}}{19}{figure.2.9}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Articles grouped by publisher in the UK Doi database published by by each publisher. Only the top 12 publishers are shown.}}{20}{figure.2.10}
\contentsline {figure}{\numberline {2.11}{\ignorespaces The log Frequency of words vs the log of their position in the rank in the word frequency table in blue. Best fit line in Red, gradient = -1.11, intercept 6.3. }}{21}{figure.2.11}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The training architectures of the Word2Vec training algorithm. Word vectors are denoted $v(i)$ for word i. In CBOW word i is predicted by the vector found by summing vectors surrounding i, and $v(i)$ is adjusted to be closer to this prediction. In skip-gram, word i's vector is pairwise compared to its context words, here i-1 and i+1 as a basis to improve $v(i)$.). CBOW attemtps to make words similar the sum of the surrounding words, skipgram attempts to minimise distance to each surrounding word.}}{24}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Schematic Representation of how concepts can be represented in word vector space. Word2Vec is able to replicate this behaviour. The vector found by vec(‘King’)- vec(‘Man’)+vec(‘Woman’) is approximately equal to vec(‘Queen’). The model has been tested on thousands of similar examples\cite {word2vec2}\cite {word2veckingqueen}.}}{25}{figure.3.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces All the punctuation removed in scraping. Only these were found in appreciable quantities in the training dataset. }}{28}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces All documents in the training database were preprocessed with this pipeline schema before being used in training models}}{30}{figure.4.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces PCA map of 10,000 documents in the corpus. PCA has not any particular structure. The dimensional reduction task is probably too difficult for PCA.}}{38}{figure.5.1}
\contentsline {figure}{\numberline {5.2}{\ignorespaces TSNE map of the same 10,000 documents. Document vectors have gathered into noticeable clusters, with non negligible outlier documents between clusters. }}{38}{figure.5.2}
\contentsline {figure}{\numberline {5.3}{\ignorespaces A Network visualisation of the 10,000 document sample. Nodes (blue) are spatially distibuted by modelling the edges (purple) as springs connecting nodes with spring constants equal to cosine similarity, then allowing the system to approach equilibrium. Edges were only placed between nodes with cosine similarity greater than 0.35 for computational tractability. The edges have been curved to aid visualisation.}}{40}{figure.5.3}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces A Network visualisation of the CCD. Edges were placed between nodes with weights corresponding to cosine similarity if $S_{cosine}$. Nodes are coloured by their detected communities, and node size is proportional to the number of connections a node has. nodes are arranged by modelling edges as springs.}}{42}{figure.6.1}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Recursion Tree for how communities were found. The dataset was partitioned using the modularity algorithm. Partitions with more than 100 documents were then repartitioned recursively. Partitions of less than 100 documents were considered to be communities (red nodes in the diagram). The figure shows the maximum depth of partition required was 8, and most communities were found after 3 partitions.}}{43}{figure.6.2}
\contentsline {figure}{\numberline {6.3}{\ignorespaces This figure shows a heatmap of author similarity. Dark pixels correspond to the author in the pixel's row having similar research interests to the author in the pixel's column. The matrix has been scaled to the range 0,1 The authors are arranged by clusters found in UPGMA. The hierarchical clustering structure is represented by the dendrogram connecting author pairs together.}}{46}{figure.6.3}
\contentsline {figure}{\numberline {6.4}{\ignorespaces The dendrogram of figure \ref {fig:AUTHORSIMS}}}{48}{figure.6.4}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Cluster labels overlayed over the distinct branches of the dendrogram.}}{49}{figure.6.5}
\contentsline {figure}{\numberline {6.6}{\ignorespaces Number of research communities authors are associated with. High values indicate an author publishing across many communities, suggest more interdisciplinary work, but also higher publication count per author. (The same plot, scaled for publication count) is included in the appendix}}{51}{figure.6.6}
\contentsline {figure}{\numberline {6.7}{\ignorespaces Heatmap showing author-author pair values for how often authors publish works in the same communities. High values indicated that Authors are predicted to have similar publication profiles. Note the authors are arranged with the ordering from figure \ref {fig:AUTHORSIMS}.}}{52}{figure.6.7}
\contentsline {figure}{\numberline {6.8}{\ignorespaces Raw collaboration matrix (values scaled to range 0,1). Note the general lack of co-publishing between staff members. Again staff are ordered by clustering described in section \ref {sec:AUTHORCLUSTERS}, but no actual clustering has been performed. Hot spots near the diagonal suggest that author pairs clustered together in \ref {sec:AUTHORCLUSTERS} generally collaborate more than distant author pairs. }}{54}{figure.6.8}
\contentsline {figure}{\numberline {6.9}{\ignorespaces Matrix formed by summing collaboration of author pairs over research communities (values scaled to range 0,1). Qualitatively similar to \ref {fig:rawcollabs}. Hot spots near diagonal again suggest authors closely clustered in section \ref {sec:AUTHORCLUSTERS} collaborate more frequently }}{55}{figure.6.9}
\contentsline {figure}{\numberline {6.10}{\ignorespaces Recommending matrix. High values (Deep red) indicate authors that have similar research but for which there is little evidence of collaboration on published works. Values $\sim $ 0 (grey/white) are where authors are neither similar nor collaborate, or are similar and collaborate closely. Values towards -1, (Blue) indicate authors that are collaborate but do share similar research (not strongly observed, as expected. High negative values would be somewhat paradoxical.) }}{56}{figure.6.10}
\contentsline {figure}{\numberline {6.11}{\ignorespaces Recommendations for a particular staff member from the recommendation matrix, plotted in bar form, (Professor Goodman). (Bars very close to zero have been removed). }}{57}{figure.6.11}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {9.1}{\ignorespaces Number of research communities authors are associated with. High values suggest more interdisciplinary work. The bars have been scaled relative to the author's total publication count. }}{67}{figure.9.1}
